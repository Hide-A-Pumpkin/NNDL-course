{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2-vgg accuracy compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/newdisk/zxy/zxy_virtualenv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm as tqdm\n",
    "from IPython import display\n",
    "\n",
    "from model.vgg import VGG_A\n",
    "# from model.vgg import VGG_A_Light\n",
    "from model.vgg_batchnorm import VGG_A_BatchNorm\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "tensor([[[ 0.1373,  0.1843,  0.1529,  ..., -0.5529, -0.6000, -0.6627],\n",
      "         [ 0.3255,  0.3725,  0.3255,  ..., -0.2078, -0.2000, -0.2863],\n",
      "         [ 0.3882,  0.4353,  0.4275,  ...,  0.3333,  0.3804,  0.2392],\n",
      "         ...,\n",
      "         [ 0.3412,  0.3804,  0.4275,  ...,  0.6157,  0.6000,  0.4039],\n",
      "         [ 0.3176,  0.3490,  0.3961,  ...,  0.6000,  0.5765,  0.3804],\n",
      "         [ 0.4275,  0.4510,  0.4824,  ...,  0.5373,  0.5294,  0.3490]],\n",
      "\n",
      "        [[ 0.1216,  0.1608,  0.1608,  ..., -0.4275, -0.4667, -0.5451],\n",
      "         [ 0.2706,  0.3098,  0.2941,  ..., -0.1686, -0.1686, -0.2627],\n",
      "         [ 0.3255,  0.3647,  0.3725,  ...,  0.3412,  0.3569,  0.2078],\n",
      "         ...,\n",
      "         [ 0.2784,  0.3176,  0.3569,  ...,  0.4588,  0.4431,  0.2627],\n",
      "         [ 0.2235,  0.2471,  0.2941,  ...,  0.4431,  0.4118,  0.2392],\n",
      "         [ 0.2784,  0.2941,  0.3255,  ...,  0.3882,  0.3804,  0.2078]],\n",
      "\n",
      "        [[ 0.1137,  0.1294,  0.1059,  ..., -0.3647, -0.4039, -0.4824],\n",
      "         [ 0.2627,  0.2784,  0.2392,  ..., -0.1451, -0.1529, -0.2471],\n",
      "         [ 0.2392,  0.2627,  0.2549,  ...,  0.3020,  0.3255,  0.1843],\n",
      "         ...,\n",
      "         [ 0.1373,  0.1765,  0.2157,  ...,  0.4039,  0.3882,  0.2157],\n",
      "         [ 0.0902,  0.1137,  0.1608,  ...,  0.3804,  0.3569,  0.1843],\n",
      "         [ 0.1608,  0.1765,  0.2157,  ...,  0.3020,  0.2863,  0.1294]]])\n",
      "tensor(7)\n",
      "torch.Size([3, 32, 32])\n",
      "tensor(0.7647)\n",
      "tensor(-0.6863)\n"
     ]
    }
   ],
   "source": [
    "# train and evaluation function\n",
    "\n",
    "# ## Constants (parameters) initialization\n",
    "device_id = [0,1,2,3]\n",
    "num_workers = 4\n",
    "batch_size = 128\n",
    "\n",
    "# add our package dir to path \n",
    "module_path = os.path.dirname(os.getcwd())\n",
    "home_path = module_path\n",
    "figures_path = os.path.join(home_path, 'reports', 'figures')\n",
    "models_path = os.path.join(home_path, 'reports', 'models')\n",
    "\n",
    "# Make sure you are using the right device.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "\n",
    "transform = transforms.Compose( [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "\n",
    "# # load dataset from cifar10\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
    "\n",
    "validset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "val_loader = torch.utils.data.DataLoader(validset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "for X,y in train_loader:\n",
    "    print(X[0])\n",
    "    print(y[0])\n",
    "    print(X[0].shape)\n",
    "    img = np.transpose(X[0], [1,2,0])\n",
    "    plt.imshow(img*0.5 + 0.5)\n",
    "    plt.savefig('sample.png')\n",
    "    print(X[0].max())\n",
    "    print(X[0].min())\n",
    "    break\n",
    "\n",
    "\n",
    "\n",
    "# This function is used to calculate the accuracy of model classification\n",
    "def get_accuracy(pred,y):\n",
    "    return pred.eq(y.view_as(pred)).sum().item()\n",
    "\n",
    "# Set a random seed to ensure reproducible results\n",
    "def set_random_seeds(seed_value=0, device='cpu'):\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    if device != 'cpu': \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# train function\n",
    "def train(model, optimizer, criterion, train_loader, val_loader, scheduler=None, epochs_n=100, best_model_path=None):\n",
    "    model.to(device)\n",
    "    learning_curve = [0] * epochs_n\n",
    "    train_accuracy_curve = [0] * epochs_n\n",
    "    val_accuracy_curve = [0] * epochs_n\n",
    "    max_val_accuracy = 0\n",
    "    max_val_accuracy_epoch = 0\n",
    "\n",
    "    batches_n = len(train_loader)\n",
    "    losses_list = []\n",
    "    grads = []\n",
    "    for epoch in tqdm(range(epochs_n), unit='epoch'):\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        model.train()\n",
    "\n",
    "        loss_list = []  # use this to record the loss value of each step\n",
    "        grad = []  # use this to record the loss gradient of each step\n",
    "        learning_curve[epoch] = 0  # maintain this to plot the training curve\n",
    "\n",
    "        for data in train_loader:\n",
    "            x, y = data\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            prediction = model(x)\n",
    "            loss = criterion(prediction, y)\n",
    "            loss_list.append(loss.item())\n",
    "            temp = model.classifier[4].weight.grad\n",
    "            grad.append(temp)\n",
    "            pred = prediction.argmax(dim = 1)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        losses_list.append(loss_list)\n",
    "        grads.append(grad)\n",
    "        display.clear_output(wait=True)\n",
    "        f, axes = plt.subplots(1, 2, figsize=(15, 3))\n",
    "\n",
    "        learning_curve[epoch] /= batches_n\n",
    "        axes[0].plot(learning_curve)\n",
    "\n",
    "        model.eval()\n",
    "        batches_n = len(val_loader.dataset)\n",
    "        for data in val_loader:\n",
    "            x, y = data\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            prediction = model(x)\n",
    "            loss = criterion(prediction, y)\n",
    "            pred = prediction.argmax(dim = 1)\n",
    "            val_accuracy_curve[epoch] += get_accuracy(pred,y)\n",
    "            \n",
    "        val_accuracy_curve[epoch]  = val_accuracy_curve[epoch] /batches_n\n",
    "        if max_val_accuracy < val_accuracy_curve[epoch]:\n",
    "            max_val_accuracy = val_accuracy_curve[epoch]\n",
    "            max_val_accuracy_epoch = epoch\n",
    "        \n",
    "        print(\"epoch:{}, valid accuracy:{}, max valid accuracy:{}, max valid accuracy epoch:{}\".format(epoch, val_accuracy_curve[epoch], max_val_accuracy,max_val_accuracy_epoch))\n",
    "    \n",
    "\n",
    "    return losses_list, grads, val_accuracy_curve\n",
    "\n",
    "\n",
    "# change file save path\n",
    "epo = 20\n",
    "loss_save_path = ''\n",
    "grad_save_path = ''\n",
    "\n",
    "# set random seed here \n",
    "set_random_seeds(seed_value=2020, device=device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [04:50<00:00, 14.53s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19, valid accuracy:0.7552, max valid accuracy:0.764, max valid accuracy epoch:14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "plt.clf()\n",
    "print('----First model for picture----'+'\\n')\n",
    "model = VGG_A()\n",
    "lr = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "VGG_A_loss, VGG_A_grads, val_accuracy_curve = train(model, optimizer, criterion, train_loader, val_loader, epochs_n=epo)\n",
    "x = range(0, epo)\n",
    "y = val_accuracy_curve\n",
    "plt.plot(x, y, 'o-')\n",
    "plt.title('valid accuracy vs epoches')\n",
    "plt.ylabel('valid accuracy')\n",
    "plt.xlabel('epoches')\n",
    "plt.savefig(\"VGG_A_accuracy.jpg\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [06:06<00:00, 18.34s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19, valid accuracy:0.8245, max valid accuracy:0.8251, max valid accuracy epoch:13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('----next model for picture----'+'\\n')\n",
    "model = VGG_A_BatchNorm()\n",
    "lr = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "VGG_A_loss, VGG_A_grads, val_accuracy_curve = train(model, optimizer, criterion, train_loader, val_loader, epochs_n=epo)\n",
    "x = range(0, epo)\n",
    "y = val_accuracy_curve\n",
    "plt.plot(x, y, 'o-')\n",
    "plt.title('valid accuracy vs epoches')\n",
    "plt.ylabel('valid accuracy')\n",
    "plt.xlabel('epoches')\n",
    "plt.savefig(\"VGG_A_BatchNorm_accuracy.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loss landscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:56<00:00, 17.84s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19, valid accuracy:0.8318, max valid accuracy:0.8374, max valid accuracy epoch:12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('----First model----'+'\\n')\n",
    "\n",
    "lr_list = [1e-3, 2e-3, 1e-4, 5e-4]\n",
    "loss_list = []\n",
    "grad_list = []\n",
    "for lr in lr_list:\n",
    "    model = VGG_A()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    VGG_A_loss, VGG_A_grads, val_accuracy_curve = train(model, optimizer, criterion, train_loader, val_loader, epochs_n=epo)\n",
    "    #np.savetxt(os.path.join(loss_save_path, 'loss.txt'), VGG_A_loss, fmt='%s', delimiter=' ')\n",
    "    #np.savetxt(os.path.join(grad_save_path, 'grads.txt'), VGG_A_grads, fmt='%s', delimiter=' ')\n",
    "    loss_list.append(VGG_A_loss)\n",
    "    grad_list.append(VGG_A_grads)\n",
    "\n",
    "min_curve = []\n",
    "max_curve = []\n",
    "\n",
    "for epoch in range(epo):\n",
    "    epochlen = len(loss_list[0][epoch])\n",
    "    for loss in range(epochlen):\n",
    "        max_loss = max(loss_list[0][epoch][loss],loss_list[1][epoch][loss],loss_list[2][epoch][loss],loss_list[3][epoch][loss])\n",
    "        max_curve.append(max_loss)\n",
    "        min_loss = min(loss_list[0][epoch][loss],loss_list[1][epoch][loss],loss_list[2][epoch][loss],loss_list[3][epoch][loss])\n",
    "        min_curve.append(min_loss)\n",
    "    \n",
    "\n",
    "print('----Next model----'+'\\n')\n",
    "\n",
    "lr_list = [1e-3, 2e-3, 1e-4, 5e-4]\n",
    "loss_list = []\n",
    "grad_list = []\n",
    "for lr in lr_list:\n",
    "    model = VGG_A_BatchNorm()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    VGG_A_loss, VGG_A_grads, val_accuracy_curve = train(model, optimizer, criterion, train_loader, val_loader, epochs_n=epo)\n",
    "    loss_list.append(VGG_A_loss)\n",
    "    grad_list.append(VGG_A_grads)\n",
    "\n",
    "\n",
    "min_curve_BN = []\n",
    "max_curve_BN = []\n",
    "for epoch in range(epo):\n",
    "    eplen = len(loss_list[0][epoch])\n",
    "    for loss in range(eplen):\n",
    "        max_loss = max(loss_list[0][epoch][loss],loss_list[1][epoch][loss],loss_list[2][epoch][ele],loss_list[3][epoch][ele])\n",
    "        max_curve_BN.append(max_loss)\n",
    "        min_loss = min(loss_list[0][epoch][ele],loss_list[1][epoch][ele],loss_list[2][epoch][ele],loss_list[3][epoch][ele])\n",
    "        min_curve_BN.append(min_loss)\n",
    "\n",
    "def write_file(ls,fname):\n",
    "    f = open(fname, \"w\",encoding='UTF-8')\n",
    "    i = 0\n",
    "    for ele in ls:\n",
    "        i = i+1\n",
    "        f.write(str(ele)+'\\t')\n",
    "        if i % 100 == 0:\n",
    "            f.write('\\n')\n",
    "    f.close()\n",
    "    \n",
    "write_file(min_curve_BN,'min_curve_BN.txt')\n",
    "write_file(max_curve_BN,'max_curve_BN.txt')\n",
    "write_file(min_curve,'min_curve.txt')\n",
    "write_file(max_curve,'max_curve.txt')\n",
    "# Use this function to plot the final loss landscape,\n",
    "# fill the area between the two curves can use plt.fill_between()\n",
    "# def plot_loss_landscape(min_curve_BN, max_curve_BN, min_curve, max_curve):\n",
    "#     x = list(range(len(min_curve)))\n",
    "#     x = np.array(x) \n",
    "#     min_curve_BN = np.array(min_curve_BN) \n",
    "#     max_curve_BN = np.array(max_curve_BN) \n",
    "#     min_curve = np.array(min_curve) \n",
    "#     max_curve = np.array(max_curve) \n",
    "    \n",
    "#     plt.plot(x, min_curve, 'g')\n",
    "#     plt.plot(x, max_curve, 'g')\n",
    "#     p1 = plt.fill_between(x, min_curve, max_curve, facecolor=\"green\", alpha=0.3)\n",
    "    \n",
    "#     plt.plot(x, min_curve_BN, 'r')\n",
    "#     plt.plot(x, max_curve_BN, 'r')\n",
    "#     p2 = plt.fill_between(x, min_curve_BN, max_curve_BN, facecolor=\"red\", alpha=0.3)\n",
    "    \n",
    "#     l1 = plt.legend([p1, p2], [\"VGG_A\", \"VGG_A_BatchNorm\"], loc='upper right')\n",
    "#     plt.title('Loss_landscape vs Steps')\n",
    "#     plt.ylabel('Loss_landscape')\n",
    "#     plt.xlabel('Steps')\n",
    "#     plt.savefig(\"Loss_landscape_update.jpg\")\n",
    "#     plt.gca().add_artist(l1)\n",
    "    \n",
    "\n",
    "# plot_loss_landscape(min_curve_BN, max_curve_BN, min_curve, max_curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grad_landscape(min_curve_BN, max_curve_BN, min_curve, max_curve):\n",
    "    x = list(range(len(min_curve)))\n",
    "    x = np.array(x) \n",
    "    min_curve_BN = np.array(min_curve_BN) \n",
    "    max_curve_BN = np.array(max_curve_BN) \n",
    "    min_curve = np.array(min_curve) \n",
    "    max_curve = np.array(max_curve) \n",
    "    \n",
    "    ax1 = plt.subplot(1, 2, 1, frameon = False)\n",
    "    plt.plot(x, min_curve, color = '#DB7093')\n",
    "    plt.plot(x, max_curve, color = '#DB7093')\n",
    "    p1 = plt.fill_between(x, min_curve, max_curve, facecolor=\"green\", alpha=0.1)\n",
    "    plt.title('Standard VGG')\n",
    "    plt.ylabel('grad_landscape')\n",
    "    plt.xlabel('Steps')\n",
    "    \n",
    "    plt.ylim((0, 6))\n",
    "    \n",
    "    ax2 = plt.subplot(1, 2, 2, frameon = False)\n",
    "    plt.plot(x, min_curve_BN, color = '#98FB98')\n",
    "    plt.plot(x, max_curve_BN, color = '#98FB98')\n",
    "    p2 = plt.fill_between(x, min_curve_BN, max_curve_BN, facecolor=\"red\", alpha=0.1)\n",
    "    \n",
    "    \n",
    "    plt.title('Standard VGG + BatchNorm')\n",
    "    plt.ylabel('grad_landscape')\n",
    "    plt.xlabel('Steps')\n",
    "    \n",
    "    plt.ylim((0, 6))\n",
    "    plt.savefig(\"grad_landscape.jpg\")\n",
    "\n",
    "plot_grad_landscape(min_curve_BN, max_curve_BN, min_curve, max_curve)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0adcc2737ebf6a4a119f135174df96668767fca1ef1112612db5ecadf2b6d608"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
