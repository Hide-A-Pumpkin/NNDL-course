{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra bonus experiment\n",
    "zhao xinyi\n",
    "2022.5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/newdisk/zxy/zxy_virtualenv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from slbi_toolbox_adam import SLBI_ToolBox\n",
    "from utils import *\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from lenet5 import LeNet5\n",
    "from get_small_model import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from plot import CNNLayerVisualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 128\n",
    "max_epoch = 20\n",
    "lr = 3e-4\n",
    "kappa = 1\n",
    "mu = 20\n",
    "weight_decay = 0\n",
    "interval = 10\n",
    "betas = (0.9,0.999)\n",
    "eps = 1e-8\n",
    "torch.backends.cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seeds(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "# model = LeNet5().to(device)\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batchsize,shuffle=True, num_workers=0)\n",
    "validset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "validloader = torch.utils.data.DataLoader(validset, batch_size=batchsize,shuffle=True, num_workers=0)\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,download=True, transform=transforms.ToTensor())\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batchsize,shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, device, valid_dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    correct = 0.\n",
    "    total_len = len(valid_dataloader.dataset)\n",
    "    with torch.no_grad():\n",
    "        for idx, (data, target) in enumerate(valid_dataloader):\n",
    "            \n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data) # batch_size * 1\n",
    "            # total_loss += loss_fn(output, target).item()\n",
    "            total_loss += F.nll_loss(output, target, reduction = \"sum\").item()\n",
    "            pred = output.argmax(dim = 1)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            \n",
    "    total_loss = total_loss / total_len\n",
    "    acc = correct/total_len\n",
    "    print(\"valid loss:{}, Accuracy:{}\".format(total_loss, acc)) \n",
    "    return total_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight\n",
      "conv1.bias\n",
      "conv2.weight\n",
      "conv2.bias\n",
      "conv3.weight\n",
      "conv3.bias\n",
      "fc1.weight\n",
      "fc1.bias\n",
      "fc2.weight\n",
      "fc2.bias\n",
      "*******************************************\n",
      "lr  :  1\n",
      "kappa  :  1\n",
      "mu  :  20\n",
      "betas  :  (0.9, 0.999)\n",
      "eps  :  1e-08\n",
      "weight_decay  :  0\n",
      "dampening  :  0\n",
      "*******************************************\n",
      "num of all step: 9380\n",
      "***********************************\n",
      "learning rate: 0.0003\n",
      "***********************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/newdisk/zxy/pj2/codes_for_pj/section3/mnist/slbi_opt_adam.py:99: UserWarning: This overload of addcdiv_ is deprecated:\n",
      "\taddcdiv_(Number value, Tensor tensor1, Tensor tensor2)\n",
      "Consider using one of the following signatures instead:\n",
      "\taddcdiv_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1055.)\n",
      "  p.data.addcdiv_(-step_size, exp_avg, denom)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss:0.08741218917965889, Accuracy:0.9735\n",
      "***********************************\n",
      "learning rate: 0.0003\n",
      "***********************************\n",
      "valid loss:0.06728858625888824, Accuracy:0.9792\n",
      "***********************************\n",
      "learning rate: 0.0003\n",
      "***********************************\n",
      "valid loss:0.06017968087270856, Accuracy:0.9798\n",
      "***********************************\n",
      "learning rate: 0.0003\n",
      "***********************************\n",
      "valid loss:0.05451583837270737, Accuracy:0.9819\n",
      "***********************************\n",
      "learning rate: 0.0003\n",
      "***********************************\n",
      "valid loss:0.0587643620878458, Accuracy:0.9808\n",
      "***********************************\n",
      "learning rate: 0.0003\n",
      "***********************************\n",
      "valid loss:0.042018158844858405, Accuracy:0.987\n",
      "***********************************\n",
      "learning rate: 0.0003\n",
      "***********************************\n",
      "valid loss:0.04255236992910504, Accuracy:0.987\n",
      "***********************************\n",
      "learning rate: 0.0003\n",
      "***********************************\n",
      "valid loss:0.04338330314643681, Accuracy:0.9859\n",
      "***********************************\n",
      "learning rate: 0.0003\n",
      "***********************************\n",
      "valid loss:0.03751743003353476, Accuracy:0.9878\n",
      "***********************************\n",
      "learning rate: 0.0003\n",
      "***********************************\n",
      "valid loss:0.03384944795966149, Accuracy:0.9897\n",
      "***********************************\n",
      "learning rate: 2.9999999999999997e-05\n",
      "***********************************\n",
      "valid loss:0.03081426266655326, Accuracy:0.9906\n",
      "***********************************\n",
      "learning rate: 2.9999999999999997e-05\n",
      "***********************************\n",
      "valid loss:0.0315433008685708, Accuracy:0.9896\n",
      "***********************************\n",
      "learning rate: 2.9999999999999997e-05\n",
      "***********************************\n",
      "valid loss:0.030100487026805057, Accuracy:0.9909\n",
      "***********************************\n",
      "learning rate: 2.9999999999999997e-05\n",
      "***********************************\n",
      "valid loss:0.030982504306919872, Accuracy:0.9893\n",
      "***********************************\n",
      "learning rate: 2.9999999999999997e-05\n",
      "***********************************\n",
      "valid loss:0.03082396769449115, Accuracy:0.9898\n",
      "***********************************\n",
      "learning rate: 2.9999999999999997e-05\n",
      "***********************************\n",
      "valid loss:0.03015963465720415, Accuracy:0.9905\n",
      "***********************************\n",
      "learning rate: 2.9999999999999997e-05\n",
      "***********************************\n",
      "valid loss:0.03000130101814866, Accuracy:0.9907\n",
      "***********************************\n",
      "learning rate: 2.9999999999999997e-05\n",
      "***********************************\n",
      "valid loss:0.030855865363928023, Accuracy:0.9898\n",
      "***********************************\n",
      "learning rate: 2.9999999999999997e-05\n",
      "***********************************\n",
      "valid loss:0.031872897874331105, Accuracy:0.9901\n",
      "***********************************\n",
      "learning rate: 2.9999999999999997e-05\n",
      "***********************************\n",
      "valid loss:0.03069872915893793, Accuracy:0.9903\n",
      "Correct :  9903\n",
      "Num :  10000\n",
      "Test ACC :  0.9903\n",
      "Top 5 ACC :  1.0\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "model = LeNet5().cuda()\n",
    "load_pth = torch.load('lenet.pth')\n",
    "model.load_state_dict(load_pth['model'])\n",
    "name_list = []\n",
    "layer_list = []\n",
    "for name, p in model.named_parameters():\n",
    "    name_list.append(name)\n",
    "    print(name)\n",
    "    if len(p.data.size()) == 4 or len(p.data.size()) == 2:\n",
    "        layer_list.append(name)\n",
    "optimizer = SLBI_ToolBox(model.parameters(), lr=1, kappa=kappa, mu=mu, weight_decay=weight_decay)\n",
    "# optimizer = SLBI_ToolBox(model.parameters(), lr=lr, kappa=kappa, mu=mu, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "# optimizer =  torch.optim.SGD(model.parameters(), lr = 0.04, momentum=0.5,weight_decay=weight_decay)\n",
    "# optimizer.assign_name(name_list)\n",
    "# optimizer.initialize_slbi(layer_list)\n",
    "\n",
    "all_num = max_epoch * len(trainloader)\n",
    "\n",
    "total_loss = []\n",
    "acc = []\n",
    "valid_loss = []\n",
    "valid_acc = []\n",
    "\n",
    "print('num of all step:', all_num)\n",
    "# print('num of step per epoch:', len(trainloader))\n",
    "for ep in range(max_epoch):\n",
    "    model.train()\n",
    "    descent_lr(lr, ep, optimizer, interval)\n",
    "    loss_val = 0.\n",
    "    correct = num = 0\n",
    "    total_len = len(trainloader.dataset)\n",
    "\n",
    "    for iter, pack in enumerate(trainloader):\n",
    "        data, target = pack[0].to(device), pack[1].to(device)\n",
    "        logits = model(data)\n",
    "        loss = F.nll_loss(logits, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        _, pred = logits.max(1)\n",
    "        loss_val += loss.item()\n",
    "        correct += pred.eq(target).sum().item()\n",
    "        num += data.shape[0]            \n",
    "        if (iter + 1) % 1000 == 0:\n",
    "            # print('*******************************')\n",
    "            print('epoch : ', ep + 1)\n",
    "            print('iteration : ', iter + 1,'loss : ', loss_val)\n",
    "            print('Train accuracy : ', correct/num)\n",
    "            loss_val = 0\n",
    "            correct = num = 0\n",
    "        \n",
    "    total_loss.append(loss_val/num)\n",
    "    acc.append(correct/num)\n",
    "\n",
    "    total_loss_0, acc_0 = evaluate(model, device, validloader)\n",
    "    valid_loss.append(total_loss_0)\n",
    "    valid_acc.append(acc_0)\n",
    "\n",
    "    optimizer.update_prune_order(ep)\n",
    "\n",
    "evaluate_batch(model, testloader, device)\n",
    "\n",
    "save_model_and_optimizer(model, optimizer, 'lenet_sgd.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss curve\n",
    "evaluate_batch(model, testloader, device)\n",
    "import matplotlib.pyplot as plt\n",
    "x1 = range(0, max_epoch)\n",
    "x2 = range(0, max_epoch)\n",
    "y1 = acc\n",
    "y2 = total_loss\n",
    "y3 = valid_loss\n",
    "y4 = valid_acc\n",
    "\n",
    "# 绘制结果\n",
    "plt.clf()\n",
    "plt.subplot(2, 1, 1)\n",
    "# plt.ylim(0.4,1)\n",
    "plt.plot(x1, y1, '.-')\n",
    "plt.plot(x1, y4, '.-')\n",
    "\n",
    "\n",
    "\n",
    "plt.title('train loss vs. epoches')\n",
    "plt.ylabel('accuracy')\n",
    "\n",
    "\n",
    "    \n",
    "l1 = plt.legend([\"train\", \"valid\"], loc='lower right')\n",
    "plt.gca().add_artist(l1)\n",
    "\n",
    "# plt.subplot(2, 1, 2)\n",
    "# plt.plot(x2, y2, '.-')\n",
    "# plt.plot(x2, y3, '.-')\n",
    "# plt.xlabel('epoches')\n",
    "# plt.ylabel('train loss')\n",
    "\n",
    "plt.savefig(\"dessibli+adam_accuracy.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot import CNNLayerVisualization\n",
    "\n",
    "for index in range(20):\n",
    "    cnn_layer = \"c5\"#useless\n",
    "    filter_pos =index\n",
    "    pretrained_model = get_lenet()\n",
    "    \n",
    "    # Fully connected layer is not needed\n",
    "    # pretrained_model = models.resnet18(pretrained=True)\n",
    "    \n",
    "    print(type(pretrained_model))\n",
    "    print(pretrained_model)\n",
    "    linear=pretrained_model.conv3.weight.data\n",
    "    print(linear.shape)\n",
    "    linear=torch.reshape(linear,[120,-1])\n",
    "    linear=torch.sum(linear*linear,dim=1)\n",
    "    print(linear)\n",
    "    for i in range(len(linear)):\n",
    "        # if linear[i]>0.0 and linear[i]<1:\n",
    "        #     continue\n",
    "        print(\"filter index\",i,\"\\t\",\"filter norm{:.4f}\".format(linear[i]))\n",
    "    print(index)\n",
    "\n",
    "    layer_vis = CNNLayerVisualization(pretrained_model, cnn_layer, filter_pos)\n",
    "\n",
    "    # Layer visualization with pytorch hooks\n",
    "    layer_vis.visualise_layer_with_hooks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************************************\n",
      "lr  :  0.01\n",
      "kappa  :  1\n",
      "mu  :  20\n",
      "betas  :  (0.9, 0.999)\n",
      "eps  :  1e-08\n",
      "weight_decay  :  0\n",
      "dampening  :  0\n",
      "*******************************************\n",
      "prune conv3\n",
      "acc before pruning\n",
      "Correct :  9706\n",
      "Num :  10000\n",
      "Test ACC :  0.9706\n",
      "Top 5 ACC :  0.9998\n",
      "acc after pruning\n",
      "conv3.weight\n",
      "conv3.bias\n",
      "Correct :  8925\n",
      "Num :  10000\n",
      "Test ACC :  0.8925\n",
      "Top 5 ACC :  0.9983\n",
      "acc after pruning conv3+fc1\n",
      "conv3.weight\n",
      "conv3.bias\n",
      "fc1.weight\n",
      "Correct :  8837\n",
      "Num :  10000\n",
      "Test ACC :  0.8837\n",
      "Top 5 ACC :  0.998\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8837"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test prune one layer\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "# load_pth = torch.load('lenet_sgd.pth')\n",
    "load_pth = torch.load('lenet_dessilbi.pth')\n",
    "torch.cuda.empty_cache()\n",
    "model = LeNet5().cuda()\n",
    "model.load_state_dict(load_pth['model'])\n",
    "name_list = []\n",
    "layer_list = []\n",
    "for name, p in model.named_parameters():\n",
    "    name_list.append(name)\n",
    "    if len(p.data.size()) == 4 or len(p.data.size()) == 2:\n",
    "        layer_list.append(name)\n",
    "\n",
    "optimizer = SLBI_ToolBox(model.parameters(), lr=1e-2, kappa=1, mu=20, weight_decay=0)\n",
    "optimizer.load_state_dict(load_pth['optimizer'])\n",
    "optimizer.assign_name(name_list)\n",
    "optimizer.initialize_slbi(layer_list)\n",
    "\n",
    "print('prune conv3')\n",
    "print('acc before pruning')\n",
    "evaluate_batch(model, testloader, 'cuda')\n",
    "print('acc after pruning')\n",
    "optimizer.prune_layer_by_order_by_name(60, 'conv3.weight', True)\n",
    "evaluate_batch(model, testloader, 'cuda')\n",
    "\n",
    "torch.save(model.state_dict(),'lenet_prune.pth')\n",
    "\n",
    "print('acc after pruning conv3+fc1')\n",
    "optimizer.prune_layer_by_order_by_list(60, ['conv3.weight', 'fc1.weight'], True)\n",
    "evaluate_batch(model, testloader, 'cuda')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9f94c22818100bcf418777d014cb985992422586d7083e5bbade2e4897ac34b6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('zxy_virtualenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
